{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsBGBjaBEgPl"
      },
      "source": [
        "# **Train YOLOv7 ML Object Detection Model on RGB-TIR  Dataset**\n",
        "\n",
        "![GMU](https://imgur.com/Uv876l3.png)\n",
        "\n",
        "\n",
        "Code based on the [YOLOv7 repository](https://github.com/WongKinYiu/yolov7) by WongKinYiu. \n",
        "\n",
        "Notebook modified to train custom RGB, TIR and RGB-TIR fused ML object detection models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZpGQmgIE4ru"
      },
      "source": [
        "## **1. Installing Dependencies**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsaSUvj6FX98"
      },
      "source": [
        "### 1.1 Mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQhvetA-L3Z-"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-coGSPIFkO6"
      },
      "source": [
        "# 1.2 Installing our dependencies\n",
        "\n",
        "**Note**: While installing dependencies it will prompt to restart runtime, don't worry just restart it and only run the above **1.1 Mounting google drive** cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGQkL0hkMEC2"
      },
      "outputs": [],
      "source": [
        "# Download YOLOv7 repository and install requirements\n",
        "\n",
        "%cd /content/gdrive/MyDrive\n",
        "!git clone https://github.com/augmentedstartups/yolov7.git\n",
        "%cd yolov7\n",
        "!pip install -r requirements.txt\n",
        "!pip install roboflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNq_xvfwF65W"
      },
      "source": [
        "# **2. Getting Our Dataset**\n",
        "\n",
        "If you haven't followed the link to dataset given in description, here it is again [Trash Dataset](https://roboflow.com/as-waste)\n",
        "\n",
        "- Follow the link and sign in to your Roboflow account. If you haven't signed up before, first sign up and then sign in\n",
        "- Once you are login, click the **Download this Dataset** tab in the top right corner\n",
        "- A dialogue box will open, select the YOLOv7 format, check the **Show download code** option and press continue.\n",
        "- A download code will appear "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzNG2JFDMQ5q"
      },
      "outputs": [],
      "source": [
        "%cd /content/gdrive/MyDrive/yolov7\n",
        "\n",
        "#### ROBOFLOW DATASET DOWNLOAD CODE #####\n",
        "\n",
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"WsvobDWpoRQrgokVheyL\")\n",
        "project = rf.workspace(\"george-mason-university\").project(\"2-fhjpl\")\n",
        "dataset = project.version(1).download(\"yolov7\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYAA-B5fKLW6"
      },
      "source": [
        "# **3. Run YOLOv7 Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieCqTaadHMwe"
      },
      "source": [
        "# 3.1 Getting our pretrained model, you can choose any model from below to fine-tune\n",
        "\n",
        "**Uncomment the model you want to finetune**\n",
        "\n",
        "There are five available model, uncomment the one which you want to train. For this we will be finetuning **yolov7.pt** model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnoD0kiwdcim"
      },
      "outputs": [],
      "source": [
        "%cd /content/gdrive/MyDrive/yolov7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Mz-HigZM2xo"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "wget -P /content/gdrive/MyDrive/yolov7 https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt\n",
        "#wget -P /content/gdrive/MyDrive/yolov7 https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7x.pt\n",
        "# wget -P /content/gdrive/MyDrive/yolov7 https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-w6.pt\n",
        "# wget -P /content/gdrive/MyDrive/yolov7 https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-e6.pt\n",
        "# wget -P /content/gdrive/MyDrive/yolov7 https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-d6.pt\n",
        "# wget -P /content/gdrive/MyDrive/yolov7 https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-e6e.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wm3t5iCBHq8s"
      },
      "source": [
        "# 3.2 Start Training\n",
        "\n",
        "**Note**\n",
        "\n",
        "[To get the full list of training arguments follow the link](https://github.com/WongKinYiu/yolov7/blob/main/train.py)\n",
        "\n",
        "Some important arguments to know\n",
        "- **configuration**: In the main yolov7 folder go to cfg/training folder and select the path of appropriate configuration file. Give the relative path to the file in **--cfg** argument\n",
        "- **data** the path to data folder, it will be automatically catered \n",
        "- **weights** path to pretrained weights given by **--weights** argument\n",
        "\n",
        "\n",
        "<br><br>\n",
        "\n",
        "**Note for resuming training from checkpoint** <br>\n",
        "By default, the checkpoints for the epoch are stored in folder, yolov7/runs/train, give the relative path to last epoch checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssOC8XWZN25Y"
      },
      "outputs": [],
      "source": [
        "%cd /content/gdrive/MyDrive/yolov7\n",
        "!python train.py --batch 16 --cfg cfg/training/yolov7.yaml --epochs 50 --data {dataset.location}/data.yaml --weights 'yolov7.pt' --device 0 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdyN-la5LYVm"
      },
      "source": [
        "# **4. Evaluation**\n",
        "\n",
        "- Note the checkpoints from training will be stored by default in runs/train/exp. Take the path of the latest checkpoint\n",
        "\n",
        "We can evaluate the performance of our custom training using the provided evalution script.\n",
        "\n",
        "Note we can adjust the below custom arguments. For details, see [the arguments accepted by detect.py](https://github.com/WongKinYiu/yolov7/blob/main/detect.py#L154)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the below test.py script, place the filepath to your test image and associated label in the .yaml file along with the object classes (nc). Use the weights section of the code to link the location of the weights in your google drive.   "
      ],
      "metadata": {
        "id": "IliK79X2nG3J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMbucQnL4pG7"
      },
      "outputs": [],
      "source": [
        "!python test.py --data /content/gdrive/MyDrive/yolov7/data/test.yaml --img 640 --batch 16 --conf 0.1 --iou 0.65 --device 0 --weights /content/gdrive/MyDrive/yolov7/runs/train/exp43/weights/epoch_054.pt --name test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_iqHZtn94zc"
      },
      "source": [
        "# 4.1 F1 and Precision Recall Curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYjFikI48Ngf"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "display(Image(\"/content/gdrive/MyDrive/yolov7/runs/train/exp4/F1_curve.png\", width=400, height=400))\n",
        "display(Image(\"/content/gdrive/MyDrive/yolov7/runs/train/exp4/PR_curve.png\", width=400, height=400))\n",
        "display(Image(\"/content/gdrive/MyDrive/yolov7/runs/train/exp4/confusion_matrix.png\", width=500, height=500))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pYQASsRNEKX"
      },
      "source": [
        "# 5.1.1 Run the below cell to evaluate on test images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4subGsgFOKXq"
      },
      "outputs": [],
      "source": [
        "# Run evaluation\n",
        "!python detect.py --weights /content/gdrive/MyDrive/yolov7/runs/train/exp2/weights/epoch_054.pt --conf 0.1 --source /content/gdrive/MyDrive/yolov7/military_vics_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af7q-cEONIYI"
      },
      "source": [
        "# 5.1.2 Display Inference on Folder of Test Images\n",
        "\n",
        "**Note** From the above output display copy the full path of folder where test images are stored"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79W_rpa1MGMp"
      },
      "outputs": [],
      "source": [
        "#display inference on ALL test images\n",
        "\n",
        "import glob\n",
        "from IPython.display import Image, display\n",
        "\n",
        "i = 0\n",
        "limit = 10000 # max images to print\n",
        "for imageName in glob.glob('/content/gdrive/MyDrive/yolov7/runs/detect/exp2/*.jpg'):\n",
        "    #Assuming JPG\n",
        "    if i < limit:\n",
        "      display(Image(filename=imageName))\n",
        "      print(\"\\n\")\n",
        "    i = i + 1\n",
        "\n",
        "display(Image(\"/content/gdrive/MyDrive/yolov7/military_vics_test/\", width=400, height=400))\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2EWkmufOChU"
      },
      "source": [
        "# **5.2 Now it's time to Infer on Custom Images**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pbp1q19cOrER"
      },
      "source": [
        "## 5.2.1 Helper Code For Inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tkfW_FBM4IW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/MyDrive/yolov7')\n",
        "\n",
        "\n",
        "import argparse\n",
        "import time\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.backends.cudnn as cudnn\n",
        "from numpy import random\n",
        "\n",
        "from models.experimental import attempt_load\n",
        "from utils.datasets import LoadStreams, LoadImages\n",
        "from utils.general import check_img_size, check_requirements, check_imshow, non_max_suppression, apply_classifier, \\\n",
        "    scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path\n",
        "from utils.plots import plot_one_box\n",
        "from utils.torch_utils import select_device, load_classifier, time_synchronized, TracedModel\n",
        "\n",
        "\n",
        "def letterbox(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):\n",
        "    # Resize and pad image while meeting stride-multiple constraints\n",
        "    shape = img.shape[:2]  # current shape [height, width]\n",
        "    if isinstance(new_shape, int):\n",
        "        new_shape = (new_shape, new_shape)\n",
        "\n",
        "    # Scale ratio (new / old)\n",
        "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
        "    if not scaleup:  # only scale down, do not scale up (for better test mAP)\n",
        "        r = min(r, 1.0)\n",
        "\n",
        "    # Compute padding\n",
        "    ratio = r, r  # width, height ratios\n",
        "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
        "    if auto:  # minimum rectangle\n",
        "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
        "    elif scaleFill:  # stretch\n",
        "        dw, dh = 0.0, 0.0\n",
        "        new_unpad = (new_shape[1], new_shape[0])\n",
        "        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
        "\n",
        "    dw /= 2  # divide padding into 2 sides\n",
        "    dh /= 2\n",
        "\n",
        "    if shape[::-1] != new_unpad:  # resize\n",
        "        img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
        "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
        "    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
        "    return img, ratio, (dw, dh)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iis5R9nO6kf"
      },
      "source": [
        "# 5.2.2 Configuration Parameters\n",
        "\n",
        "Change the path of both **weights** and **yaml** file\n",
        "\n",
        "**weights** will be in yolov7 main folder -> runs -> train and then select the appropriate weight\n",
        "\n",
        "**yaml** yolov7 main folder -> Trash-5, there you will find yaml file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OR69IOOpNnb4"
      },
      "outputs": [],
      "source": [
        "classes_to_filter = None  #You can give list of classes to filter by name, Be happy you don't have to put class number. ['train','person' ]\n",
        "\n",
        "\n",
        "opt  = {\n",
        "    \n",
        "    \"weights\": \"/content/gdrive/MyDrive/yolov7/runs/train/exp3/weights/best.pt\", # Path to weights file default weights are for nano model\n",
        "    \"yaml\"   : \"/content/gdrive/MyDrive/yolov7/runs/train/exp3/opt.yaml\",\n",
        "    \"img-size\": 640, # default image size\n",
        "    \"conf-thres\": 0.25, # confidence threshold for inference.\n",
        "    \"iou-thres\" : 0.45, # NMS IoU threshold for inference.\n",
        "    \"device\" : '0',  # device to run our model i.e. 0 or 0,1,2,3 or cpu\n",
        "    \"classes\" : classes_to_filter  # list of classes to filter or None\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDiFruFSPsYQ"
      },
      "source": [
        "# **5.3. Inference on Single Image**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnKI_cii2xXY"
      },
      "outputs": [],
      "source": [
        "%cd /content/gdrive/MyDrive/yolov7\n",
        "!gdown https://drive.google.com/file/d/1RCwUzdtF9b16LCfq5AD41SroYUiRQyev/view?usp=sharing\n",
        "#This does not work in Safari Browser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReP3h5j-26rw"
      },
      "outputs": [],
      "source": [
        "source_image_path = '/content/gdrive/MyDrive/yolov7/fmtv-howitzer.jpg'\n",
        "#Change the Path Name to your file name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPtGIYnSPnvj"
      },
      "outputs": [],
      "source": [
        "# Give path of source image.\n",
        "#%cd /content/gdrive/MyDrive/yolov7\n",
        "#source_image_path = '/content/trash.png'\n",
        "\n",
        "with torch.no_grad():\n",
        "  weights, imgsz = opt['weights'], opt['img-size']\n",
        "  set_logging()\n",
        "  device = select_device(opt['device'])\n",
        "  half = device.type != 'cpu'\n",
        "  model = attempt_load(weights, map_location=device)  # load FP32 model\n",
        "  stride = int(model.stride.max())  # model stride\n",
        "  imgsz = check_img_size(imgsz, s=stride)  # check img_size\n",
        "  if half:\n",
        "    model.half()\n",
        "\n",
        "  names = model.module.names if hasattr(model, 'module') else model.names\n",
        "  colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n",
        "  if device.type != 'cpu':\n",
        "    model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))\n",
        "\n",
        "  img0 = cv2.imread(source_image_path)\n",
        "  img = letterbox(img0, imgsz, stride=stride)[0]\n",
        "  img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
        "  img = np.ascontiguousarray(img)\n",
        "  img = torch.from_numpy(img).to(device)\n",
        "  img = img.half() if half else img.float()  # uint8 to fp16/32\n",
        "  img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
        "  if img.ndimension() == 3:\n",
        "    img = img.unsqueeze(0)\n",
        "\n",
        "  # Inference\n",
        "  t1 = time_synchronized()\n",
        "  pred = model(img, augment= False)[0]\n",
        "\n",
        "  # Apply NMS\n",
        "  classes = None\n",
        "  if opt['classes']:\n",
        "    classes = []\n",
        "    for class_name in opt['classes']:\n",
        "\n",
        "      classes.append(opt['classes'].index(class_name))\n",
        "\n",
        "\n",
        "  pred = non_max_suppression(pred, opt['conf-thres'], opt['iou-thres'], classes= classes, agnostic= False)\n",
        "  t2 = time_synchronized()\n",
        "  for i, det in enumerate(pred):\n",
        "    s = ''\n",
        "    s += '%gx%g ' % img.shape[2:]  # print string\n",
        "    gn = torch.tensor(img0.shape)[[1, 0, 1, 0]]\n",
        "    if len(det):\n",
        "      det[:, :4] = scale_coords(img.shape[2:], det[:, :4], img0.shape).round()\n",
        "\n",
        "      for c in det[:, -1].unique():\n",
        "        n = (det[:, -1] == c).sum()  # detections per class\n",
        "        s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
        "    \n",
        "      for *xyxy, conf, cls in reversed(det):\n",
        "\n",
        "        label = f'{names[int(cls)]} {conf:.2f}'\n",
        "        plot_one_box(xyxy, img0, label=label, color=colors[int(cls)], line_thickness=3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkBIq12rQPsY"
      },
      "outputs": [],
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "cv2_imshow(img0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6jTZk9DRJtH"
      },
      "source": [
        "# **6. Inference on Video**\n",
        "\n",
        "**Note** Make sure to make relevant changes in arguments in argument section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcLJFaVqR43h"
      },
      "source": [
        "# 6.1.1 Upload video from Local System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6kzH4UMQg30"
      },
      "outputs": [],
      "source": [
        "%cd /content/gdrive/MyDrive/yolov7\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-nJIKF_SDS0"
      },
      "source": [
        "# 6.1.2 Download video from Google Drive Link"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlW2X3qmSHxD"
      },
      "outputs": [],
      "source": [
        "#change URL\n",
        "%cd /content/gdrive/MyDrive/gallagher_oughton_project_eos/test videos/ground/tir1_dusk_62F_27oct22.mp4\n",
        "!gdown https://drive.google.com/file/d/1_gE27_5U8--BRxN-joIYZo9shP2YqtXS/view?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YOV8UmeSRld"
      },
      "source": [
        "# 6.1.3 Download from any public URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HZmc4kbSSos"
      },
      "outputs": [],
      "source": [
        "%cd /content/gdrive/MyDrive/yolov7\n",
        "! wget PUBLIC_URL_TO_MP4/AVI_FILE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDitGHQ3SV3e"
      },
      "source": [
        "# 6.1.4 Enter Video Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYrG-9imSVMJ"
      },
      "outputs": [],
      "source": [
        "#give the full path to video, your video will be in the Yolov7 folder\n",
        "video_path = '/content/gdrive/MyDrive/gallagher_oughton_project_eos/test videos/ground/tir1_dusk_62F_27oct22.mp4'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYRI2XkOSr7A"
      },
      "source": [
        "# 6.2 YOLOv7 Inference on Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Njns91pRStLZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/MyDrive/yolov7')\n",
        "\n",
        "\n",
        "import argparse\n",
        "import time\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.backends.cudnn as cudnn\n",
        "from numpy import random\n",
        "\n",
        "from models.experimental import attempt_load\n",
        "from utils.datasets import LoadStreams, LoadImages\n",
        "from utils.general import check_img_size, check_requirements, check_imshow, non_max_suppression, apply_classifier, \\\n",
        "    scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path\n",
        "from utils.plots import plot_one_box\n",
        "from utils.torch_utils import select_device, load_classifier, time_synchronized, TracedModel\n",
        "\n",
        "# Initializing video object\n",
        "video = cv2.VideoCapture(video_path)\n",
        "\n",
        "\n",
        "#Video information\n",
        "fps = video.get(cv2.CAP_PROP_FPS)\n",
        "w = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "h = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "nframes = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "# Initialzing object for writing video output\n",
        "output = cv2.VideoWriter('output_tir1.mp4', cv2.VideoWriter_fourcc(*'DIVX'),fps , (w,h))\n",
        "torch.cuda.empty_cache()\n",
        "# Initializing model and setting it for inference\n",
        "with torch.no_grad():\n",
        "  weights, imgsz = opt['weights'], opt['img-size']\n",
        "  set_logging()\n",
        "  device = select_device(opt['device'])\n",
        "  half = device.type != 'cpu'\n",
        "  model = attempt_load(weights, map_location=device)  # load FP32 model\n",
        "  stride = int(model.stride.max())  # model stride\n",
        "  imgsz = check_img_size(imgsz, s=stride)  # check img_size\n",
        "  if half:\n",
        "    model.half()\n",
        "\n",
        "  names = model.module.names if hasattr(model, 'module') else model.names\n",
        "  colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n",
        "  if device.type != 'cpu':\n",
        "    model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))\n",
        "\n",
        "  classes = None\n",
        "  if opt['classes']:\n",
        "    classes = []\n",
        "    for class_name in opt['classes']:\n",
        "      classes.append(opt['classes'].index(class_name))\n",
        "\n",
        "  for j in range(nframes):\n",
        "\n",
        "      ret, img0 = video.read()\n",
        "      if ret:\n",
        "        img = letterbox(img0, imgsz, stride=stride)[0]\n",
        "        img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(device)\n",
        "        img = img.half() if half else img.float()  # uint8 to fp16/32\n",
        "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
        "        if img.ndimension() == 3:\n",
        "          img = img.unsqueeze(0)\n",
        "\n",
        "        # Inference\n",
        "        t1 = time_synchronized()\n",
        "        pred = model(img, augment= False)[0]\n",
        "\n",
        "        \n",
        "        pred = non_max_suppression(pred, opt['conf-thres'], opt['iou-thres'], classes= classes, agnostic= False)\n",
        "        t2 = time_synchronized()\n",
        "        for i, det in enumerate(pred):\n",
        "          s = ''\n",
        "          s += '%gx%g ' % img.shape[2:]  # print string\n",
        "          gn = torch.tensor(img0.shape)[[1, 0, 1, 0]]\n",
        "          if len(det):\n",
        "            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], img0.shape).round()\n",
        "\n",
        "            for c in det[:, -1].unique():\n",
        "              n = (det[:, -1] == c).sum()  # detections per class\n",
        "              s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
        "    \n",
        "            for *xyxy, conf, cls in reversed(det):\n",
        "\n",
        "              label = f'{names[int(cls)]} {conf:.2f}'\n",
        "              plot_one_box(xyxy, img0, label=label, color=colors[int(cls)], line_thickness=3)\n",
        "        \n",
        "        print(f\"{j+1}/{nframes} frames processed\")\n",
        "        output.write(img0)\n",
        "      else:\n",
        "        break\n",
        "    \n",
        "\n",
        "output.release()\n",
        "video.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMw_X57gSyM4"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "import os\n",
        "\n",
        "# Input video path\n",
        "save_path = '/content/gdrive/MyDrive/yolov7/output.mp4'\n",
        "\n",
        "# Compressed video path\n",
        "compressed_path = \"/content/result_compressed.mp4\"\n",
        "\n",
        "os.system(f\"ffmpeg -i {save_path} -vcodec libx264 {compressed_path}\")\n",
        "\n",
        "# Show video\n",
        "mp4 = open(compressed_path,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQJPLimMS29q"
      },
      "source": [
        "# 6.3 Download Inference Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZ-jbXXqS0F3"
      },
      "outputs": [],
      "source": [
        "#downloading output video from google drive is faster\n",
        "from google.colab import files\n",
        "save_path = '/content/gdrive/MyDrive/yolov7/output.mp4'\n",
        "files.download(save_path) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dkb9SVPzS-kA"
      },
      "source": [
        "# **7. Inference on Webcam**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFPr_Z-0TCY9"
      },
      "source": [
        "# 7.1 Webcam Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_SSSnf_TAB4"
      },
      "outputs": [],
      "source": [
        "# import dependencies\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from google.colab.patches import cv2_imshow\n",
        "from base64 import b64decode, b64encode\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes\n",
        "\n",
        "\n",
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  \n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kkqh_CYnTE-f"
      },
      "outputs": [],
      "source": [
        "# start streaming video from webcam\n",
        "video_stream()\n",
        "# label for video\n",
        "label_html = 'Capturing...'\n",
        "# initialze bounding box to empty\n",
        "bbox = ''\n",
        "count = 0 \n",
        "\n",
        "with torch.no_grad():\n",
        "  weights, imgsz = opt['weights'], (480,640)\n",
        "  set_logging()\n",
        "  device = select_device(opt['device'])\n",
        "  half = device.type != 'cpu'\n",
        "  model = attempt_load(weights, map_location=device)  # load FP32 model\n",
        "  stride = int(model.stride.max())  # model stride\n",
        "\n",
        "  if half:\n",
        "    model.half()\n",
        "\n",
        "  names = model.module.names if hasattr(model, 'module') else model.names\n",
        "  colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n",
        "  if device.type != 'cpu':\n",
        "    model(torch.zeros(1, 3, imgsz[0], imgsz[1]).to(device).type_as(next(model.parameters())))\n",
        "  classes = None\n",
        "  if opt['classes']:\n",
        "    classes = []\n",
        "    for class_name in opt['classes']:\n",
        "      classes.append(opt['classes'].index(class_name))\n",
        "  \n",
        "  while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "    \n",
        "    img0 = js_to_image(js_reply[\"img\"])\n",
        "    bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n",
        "    img = letterbox(img0, imgsz, stride=stride)[0]\n",
        "    img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
        "    img = np.ascontiguousarray(img)\n",
        "    img = torch.from_numpy(img).to(device)\n",
        "    img = img.half() if half else img.float()  # uint8 to fp16/32\n",
        "    img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
        "    if img.ndimension() == 3:\n",
        "      img = img.unsqueeze(0)\n",
        "\n",
        "    # Inference\n",
        "    t1 = time_synchronized()\n",
        "    pred = model(img, augment= False)[0]\n",
        "\n",
        "    # Apply NMS\n",
        "    pred = non_max_suppression(pred, opt['conf-thres'], opt['iou-thres'], classes= classes, agnostic= False)\n",
        "    t2 = time_synchronized()\n",
        "    for i, det in enumerate(pred):\n",
        "      s = ''\n",
        "      s += '%gx%g ' % img.shape[2:]  # print string\n",
        "      gn = torch.tensor(img0.shape)[[1, 0, 1, 0]]\n",
        "      if len(det):\n",
        "        det[:, :4] = scale_coords(img.shape[2:], det[:, :4], img0.shape).round()\n",
        "\n",
        "        for c in det[:, -1].unique():\n",
        "          n = (det[:, -1] == c).sum()  # detections per class\n",
        "          s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
        "    \n",
        "        for *xyxy, conf, cls in reversed(det):\n",
        "\n",
        "          label = f'{names[int(cls)]} {conf:.2f}'\n",
        "          plot_one_box(xyxy, bbox_array, label=label, color=colors[int(cls)], line_thickness=3)\n",
        "    \n",
        "    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "    bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "    \n",
        "    bbox = bbox_bytes\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}